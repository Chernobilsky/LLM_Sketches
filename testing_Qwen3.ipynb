{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d0f337",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbfa445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:47:27.280229Z",
     "start_time": "2025-08-01T15:47:15.264438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value.\n",
      "\n",
      "I've been a researcher for twenty years, and I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never\n"
     ]
    }
   ],
   "source": [
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"gpt2\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False):\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"gpt2\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\"\"\"\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "    if use_chat:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    else:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"gpt2\") or \"openai-community/gpt2\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=gpt2\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47827d50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:51:36.213031Z",
     "start_time": "2025-08-01T15:51:31.945484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n",
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value.\n",
      "\n",
      "I've been a researcher for twenty years, and I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never\n"
     ]
    }
   ],
   "source": [
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3\": \"Qwen/Qwen3-0.6B\",  # [ADD] alias for convenience\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"gpt2\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False, trust_remote_code: bool = False):  # [MOD] added trust_remote_code\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only, trust_remote_code=trust_remote_code)  # [MOD] pass trust_remote_code\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "        trust_remote_code=trust_remote_code,  # [MOD]\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"gpt2\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\"\"\"\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "    if use_chat:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    else:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"gpt2\") or \"openai-community/gpt2\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=gpt2\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [ADD] allow remote code (needed by some repos)\n",
    "    p.add_argument(\"--trust-remote-code\", action=\"store_true\", help=\"Allow custom modeling code from repo (use only for trusted repos)\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=args.trust_remote_code)  # [MOD]\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=args.trust_remote_code,  # [MOD]\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9768bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:54:09.861376Z",
     "start_time": "2025-08-01T15:54:07.143782Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] Auto-enabling trust_remote_code for Qwen model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a344a76ccf47279ff455f72098a28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cab\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cab\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-0.6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'jinja2' has no attribute 'pass_eval_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 355>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_per_s: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoks_per_s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 356\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _in_notebook():\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 299\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_thinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    308\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_new_tokens,\n\u001b[0;32m    309\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdo_sample,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m )\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# Remove None entries (generate() complains)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mbuild_inputs\u001b[1;34m(tokenizer, prompt, system, enable_thinking, device)\u001b[0m\n\u001b[0;32m    193\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt})\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_thinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_thinking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menable_thinking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1640\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1639\u001b[0m template_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[1;32m-> 1640\u001b[0m rendered_chat, generation_indices \u001b[38;5;241m=\u001b[39m render_jinja_template(\n\u001b[0;32m   1641\u001b[0m     conversations\u001b[38;5;241m=\u001b[39mconversations,\n\u001b[0;32m   1642\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m   1643\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[0;32m   1644\u001b[0m     chat_template\u001b[38;5;241m=\u001b[39mchat_template,\n\u001b[0;32m   1645\u001b[0m     return_assistant_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_assistant_tokens_mask,\n\u001b[0;32m   1646\u001b[0m     continue_final_message\u001b[38;5;241m=\u001b[39mcontinue_final_message,\n\u001b[0;32m   1647\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39madd_generation_prompt,\n\u001b[0;32m   1648\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtemplate_kwargs,\n\u001b[0;32m   1649\u001b[0m )\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1652\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m rendered_chat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:481\u001b[0m, in \u001b[0;36mrender_jinja_template\u001b[1;34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    478\u001b[0m     )\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_jinja_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# We accept either JSON schemas or functions for tools. If we get functions, we convert them to schemas\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:398\u001b[0m, in \u001b[0;36m_compile_jinja_template\u001b[1;34m(chat_template)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_jinja_available():\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_chat_template requires jinja2 to be installed. Please install it using `pip install jinja2`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m     )\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAssistantTracker\u001b[39;00m(Extension):\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;66;03m# This extension is used to track the indices of assistant-generated tokens in the rendered chat\u001b[39;00m\n\u001b[0;32m    400\u001b[0m     tags \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, environment: ImmutableSandboxedEnvironment):\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# The class is only initiated by jinja.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:414\u001b[0m, in \u001b[0;36m_compile_jinja_template.<locals>.AssistantTracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m    411\u001b[0m     body \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_statements([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname:endgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m], drop_needle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mCallBlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_generation_support\u001b[39m\u001b[38;5;124m\"\u001b[39m), [], [], body)\u001b[38;5;241m.\u001b[39mset_lineno(lineno)\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;129m@jinja2\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpass_eval_context\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generation_support\u001b[39m(\u001b[38;5;28mself\u001b[39m, context: jinja2\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mEvalContext, caller: jinja2\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mMacro) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    416\u001b[0m     rv \u001b[38;5;241m=\u001b[39m caller()\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_active():\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;66;03m# Only track generation indices if the tracker is active\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'jinja2' has no attribute 'pass_eval_context'"
     ]
    }
   ],
   "source": [
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3\": \"Qwen/Qwen3-0.6B\",  # [ADD] alias for convenience\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"qwen3\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False, trust_remote_code: Optional[bool] = None):  # [MOD] default model qwen3; infer trust by model if None  # [MOD] added trust_remote_code\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    # [ADD] If trust_remote_code is not specified, auto-enable for Qwen models\n",
    "    if trust_remote_code is None:\n",
    "        try:\n",
    "            if (model.lower().startswith(\"qwen\") or \"Qwen/\" in model_id):\n",
    "                trust_remote_code = True\n",
    "        except Exception:\n",
    "            trust_remote_code = False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only, trust_remote_code=trust_remote_code)  # [MOD] pass trust_remote_code\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "        trust_remote_code=trust_remote_code,  # [MOD]\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"qwen3\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    trust_remote_code: Optional[bool] = None,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only, trust_remote_code=trust_remote_code)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\"\"\"\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "    if use_chat:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    else:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"qwen3\") or \"Qwen/Qwen3-0.6B\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=qwen3\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [ADD] allow remote code (needed by some repos)\n",
    "    p.add_argument(\"--trust-remote-code\", action=\"store_true\", help=\"Allow custom modeling code from repo (use only for trusted repos)\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # [ADD] Auto-enable trust for Qwen models if flag not set\n",
    "    trust_flag = args.trust_remote_code or (\"Qwen/\" in model_id or args.model.lower().startswith(\"qwen\"))\n",
    "    if trust_flag and not args.trust_remote_code:\n",
    "        print(\"[info] Auto-enabling trust_remote_code for Qwen model.\", file=sys.stderr)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_flag)  # [MOD] use inferred flag\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=trust_flag,  # [MOD] use inferred flag\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c3247c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:57:35.193623Z",
     "start_time": "2025-08-01T15:55:36.646349Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] Auto-enabling trust_remote_code for Qwen model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[warn] Chat template requires jinja2>=3.1; falling back to plain prompt. Error: module 'jinja2' has no attribute 'pass_eval_context'\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value. That something is a story. And I’ve been writing stories about the world, but I’m not sure if I’m doing it right. I’m not sure if I’m writing about the world or about myself. And I’m not sure if I’m writing about the world in a way that’s meaningful or just a\n"
     ]
    }
   ],
   "source": [
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3\": \"Qwen/Qwen3-0.6B\",  # [ADD] alias for convenience\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"qwen3\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False, trust_remote_code: Optional[bool] = None):  # [MOD] default model qwen3; infer trust by model if None  # [MOD] added trust_remote_code\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    # [ADD] If trust_remote_code is not specified, auto-enable for Qwen models\n",
    "    if trust_remote_code is None:\n",
    "        try:\n",
    "            if (model.lower().startswith(\"qwen\") or \"Qwen/\" in model_id):\n",
    "                trust_remote_code = True\n",
    "        except Exception:\n",
    "            trust_remote_code = False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only, trust_remote_code=trust_remote_code)  # [MOD] pass trust_remote_code\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "        trust_remote_code=trust_remote_code,  # [MOD]\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"qwen3\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    trust_remote_code: Optional[bool] = None,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only, trust_remote_code=trust_remote_code)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\n",
    "    Надёжно обрабатывает отсутствие jinja2 (требуется для chat_template):\n",
    "    при ошибке откатывается к обычной подаче prompt без шаблона и печатает предупреждение.\n",
    "    \"\"\"\n",
    "    model_inputs = None\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "\n",
    "    if use_chat:\n",
    "        try:\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "        except (ImportError, AttributeError) as e:\n",
    "            # [WARN] jinja2 старой версии или отсутствует → откатываемся на простой prompt\n",
    "            print(\"[warn] Chat template requires jinja2>=3.1; falling back to plain prompt. Error:\", e, file=sys.stderr)\n",
    "            use_chat = False\n",
    "        except Exception as e:\n",
    "            # Любая другая ошибка рендера — также откат на простой prompt\n",
    "            print(\"[warn] Failed to render chat template; falling back to plain prompt. Error:\", e, file=sys.stderr)\n",
    "            use_chat = False\n",
    "\n",
    "    if not use_chat or model_inputs is None:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"qwen3\") or \"Qwen/Qwen3-0.6B\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=qwen3\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [ADD] allow remote code (needed by some repos)\n",
    "    p.add_argument(\"--trust-remote-code\", action=\"store_true\", help=\"Allow custom modeling code from repo (use only for trusted repos)\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # [ADD] Auto-enable trust for Qwen models if flag not set\n",
    "    trust_flag = args.trust_remote_code or (\"Qwen/\" in model_id or args.model.lower().startswith(\"qwen\"))\n",
    "    if trust_flag and not args.trust_remote_code:\n",
    "        print(\"[info] Auto-enabling trust_remote_code for Qwen model.\", file=sys.stderr)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_flag)  # [MOD] use inferred flag\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=trust_flag,  # [MOD] use inferred flag\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14dd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
