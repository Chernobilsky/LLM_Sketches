{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434309eb",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b6c984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T16:39:23.332842Z",
     "start_time": "2025-06-22T16:39:20.865783Z"
    }
   },
   "outputs": [],
   "source": [
    "# critic.py ‚Äî –º–æ–¥—É–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ \"–∑–¥—Ä–∞–≤–æ—Å—Ç–∏\" —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "\n",
    "################################################################################\n",
    "# üß† 1. –ö–ª–∞—Å—Å TorchCritic ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "################################################################################\n",
    "\n",
    "class TorchCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫: Embedding ‚Üí BiLSTM ‚Üí –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —Ç–µ–∫—Å—Ç \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 64, hidden_dim: int = 128, coherence_threshold: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(2 * hidden_dim, 1)  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å\"\n",
    "        self.coherence_threshold = coherence_threshold  # –ü–æ—Ä–æ–≥, –Ω–∏–∂–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len) ‚Äî –±–∞—Ç—á —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        output: (batch_size,) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏\" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_ids)                 # ‚Üí (B, T, D)\n",
    "        _, (hidden, _) = self.encoder(embedded)              # hidden: (2, B, H)\n",
    "        concat_hidden = torch.cat((hidden[0], hidden[1]), dim=1)  # ‚Üí (B, 2H)\n",
    "        logits = self.classifier(concat_hidden)              # ‚Üí (B, 1)\n",
    "        probs = torch.sigmoid(logits)                        # ‚Üí [0, 1]\n",
    "        return probs.squeeze(1)                              # ‚Üí (B,)\n",
    "\n",
    "    def is_acceptable(self, input_ids: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ—à—ë–ª –ø–æ –ø–æ—Ä–æ–≥—É –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏.\n",
    "        input_ids: (seq_len,) ‚Äî –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            prob = self(input_ids.unsqueeze(0))  # ‚Üí (1,)\n",
    "            return prob.item() >= self.coherence_threshold\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üß™ 2. –ö–ª–∞—Å—Å CriticDataset ‚Äî Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏–∫–∞\n",
    "################################################################################\n",
    "\n",
    "class CriticDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è critic: –ø–∞—Ä—ã (—Ç–æ–∫–µ–Ω—ã, –º–µ—Ç–∫–∞),\n",
    "    –≥–¥–µ –º–µ—Ç–∫–∞ 1 ‚Äî —Ç–µ–∫—Å—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π, 0 ‚Äî –Ω–µ—Ç.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequences: List[List[int]], labels: List[int]):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üß™ 3. –§—É–Ω–∫—Ü–∏—è train_critic ‚Äî –æ–±—É—á–µ–Ω–∏–µ critic –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ\n",
    "################################################################################\n",
    "\n",
    "def train_critic(model: TorchCritic, dataset: CriticDataset, epochs: int = 5, lr: float = 1e-3):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç critic –ø–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            probs = model(batch_x)\n",
    "            loss = loss_fn(probs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "################################################################################\n",
    "# üîß 4. collate_fn ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ –±–∞—Ç—á–µ–π\n",
    "################################################################################\n",
    "\n",
    "def collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    –ü–∞–¥–¥–∏–Ω–≥ –±–∞—Ç—á–∞ –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã –ø–æ —Ç–æ–∫–µ–Ω–∞–º.\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return padded, torch.tensor(labels, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8e0a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T16:39:27.781452Z",
     "start_time": "2025-06-22T16:39:23.334636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.2659\n",
      "Epoch 2: loss = 0.0231\n",
      "Epoch 3: loss = 0.0060\n",
      "Epoch 4: loss = 0.0018\n",
      "Epoch 5: loss = 0.0009\n",
      "Acceptable? True\n"
     ]
    }
   ],
   "source": [
    "# critic.py ‚Äî –º–æ–¥—É–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ \"–∑–¥—Ä–∞–≤–æ—Å—Ç–∏\" —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "################################################################################\n",
    "# üß† 1. –ö–ª–∞—Å—Å TorchCritic ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "################################################################################\n",
    "\n",
    "class TorchCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫: Embedding ‚Üí BiLSTM ‚Üí –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —Ç–µ–∫—Å—Ç \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 64, hidden_dim: int = 128, coherence_threshold: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(2 * hidden_dim, 1)  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å\"\n",
    "        self.coherence_threshold = coherence_threshold  # –ü–æ—Ä–æ–≥, –Ω–∏–∂–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len) ‚Äî –±–∞—Ç—á —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        output: (batch_size,) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å \"–æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏\" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_ids)                 # ‚Üí (B, T, D)\n",
    "        _, (hidden, _) = self.encoder(embedded)              # hidden: (2, B, H)\n",
    "        concat_hidden = torch.cat((hidden[0], hidden[1]), dim=1)  # ‚Üí (B, 2H)\n",
    "        logits = self.classifier(concat_hidden)              # ‚Üí (B, 1)\n",
    "        probs = torch.sigmoid(logits)                        # ‚Üí [0, 1]\n",
    "        return probs.squeeze(1)                              # ‚Üí (B,)\n",
    "\n",
    "    def is_acceptable(self, input_ids: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ—à—ë–ª –ø–æ –ø–æ—Ä–æ–≥—É –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏.\n",
    "        input_ids: (seq_len,) ‚Äî –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            prob = self(input_ids.unsqueeze(0))  # ‚Üí (1,)\n",
    "            return prob.item() >= self.coherence_threshold\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üß™ 2. –ö–ª–∞—Å—Å CriticDataset ‚Äî Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏–∫–∞\n",
    "################################################################################\n",
    "\n",
    "class CriticDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è critic: –ø–∞—Ä—ã (—Ç–æ–∫–µ–Ω—ã, –º–µ—Ç–∫–∞),\n",
    "    –≥–¥–µ –º–µ—Ç–∫–∞ 1 ‚Äî —Ç–µ–∫—Å—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π, 0 ‚Äî –Ω–µ—Ç.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequences: List[List[int]], labels: List[int]):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üß™ 3. –§—É–Ω–∫—Ü–∏—è train_critic ‚Äî –æ–±—É—á–µ–Ω–∏–µ critic –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ\n",
    "################################################################################\n",
    "\n",
    "def train_critic(model: TorchCritic, dataset: CriticDataset, epochs: int = 5, lr: float = 1e-3):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç critic –ø–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            probs = model(batch_x)\n",
    "            loss = loss_fn(probs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üîß 4. collate_fn ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ –±–∞—Ç—á–µ–π\n",
    "################################################################################\n",
    "\n",
    "def collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    –ü–∞–¥–¥–∏–Ω–≥ –±–∞—Ç—á–∞ –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã –ø–æ —Ç–æ–∫–µ–Ω–∞–º.\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return padded, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üß™ 5. –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–µ–π–∫–æ–≤—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "################################################################################\n",
    "\n",
    "def make_dummy_training_data(vocab_size: int, num_samples: int = 100) -> Tuple[List[List[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–µ–π–∫–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã: –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ (—Ä–∞–Ω–¥–æ–º –∏ —à—É–º).\n",
    "    \"\"\"\n",
    "    data, labels = [], []\n",
    "    for _ in range(num_samples):\n",
    "        length = random.randint(5, 20)\n",
    "        # \"–•–æ—Ä–æ—à–∏–π\" –ø—Ä–∏–º–µ—Ä: –ø–ª–∞–≤–Ω—ã–µ, –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        good = sorted(random.sample(range(10, vocab_size // 2), length))\n",
    "        data.append(good)\n",
    "        labels.append(1)\n",
    "\n",
    "        # \"–ü–ª–æ—Ö–æ–π\" –ø—Ä–∏–º–µ—Ä: —à—É–º, –ø–æ–≤—Ç–æ—Ä—ã\n",
    "        bad = [random.randint(0, 5) for _ in range(length)]\n",
    "        data.append(bad)\n",
    "        labels.append(0)\n",
    "    return data, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = 1000\n",
    "    model = TorchCritic(vocab_size=vocab)\n",
    "    sequences, labels = make_dummy_training_data(vocab_size=vocab, num_samples=200)\n",
    "    dataset = CriticDataset(sequences, labels)\n",
    "    train_critic(model, dataset, epochs=5)\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞\n",
    "    test = torch.tensor(sequences[0])\n",
    "    print(\"Acceptable?\", model.is_acceptable(test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
