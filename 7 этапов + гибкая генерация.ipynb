{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3461d843",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64dbbecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:50:29.441107Z",
     "start_time": "2025-06-20T16:50:29.253773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 5, 1000])\n",
      "Loss: 7.0240020751953125\n",
      "Generated tokens: tensor([[103, 278, 567, 553,   9,  17, 780],\n",
      "        [765, 288, 255, 998, 577, 285, 125]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional  # === Добавлено этап 9: для аннотации generate ===\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    # === Добавлено на этапе 3: собственная реализация каузального внимания ===\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)  # [B, T, 3C]\n",
    "        qkv = qkv.view(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.transpose(1, 2)  # [B, heads, T, head_dim]\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale  # [B, heads, T, T]\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)) == 0\n",
    "        attn_weights = attn_weights.masked_fill(mask, float('-inf'))\n",
    "        attn_probs = torch.softmax(attn_weights, dim=-1)\n",
    "        attn_output = attn_probs @ v  # [B, heads, T, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "    # === Конец этапа 3 ===\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # === Добавлено на этапе 4: двухслойный FFN ===\n",
    "    def __init__(self, embed_dim, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or embed_dim * 4\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    # === Конец этапа 4 ===\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # === Добавлено на этапе 5: разводим attention и mlp параллельно ===\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = FeedForward(embed_dim)\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
    "        self.threshold = 1.0  # можно регулировать чувствительность сверки\n",
    "\n",
    "    def forward(self, x):\n",
    "        # параллельные ветви: attention и mlp\n",
    "        a = self.attn(self.ln_1(x))\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "\n",
    "        # сверка: если сильно различаются, подавим результат\n",
    "        diff = torch.norm(a - m, dim=-1, keepdim=True)  # [B, T, 1]\n",
    "        mask = (diff < self.threshold).float()\n",
    "        combined = mask * (a + m) / 2  # если различаются сильно, обнулим\n",
    "\n",
    "        return x + combined\n",
    "# === Конец модификации на этапе 8 ===\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_seq_len=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # === Добавлено на этапе 2: позиционные эмбеддинги ===\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        # === Конец этапа 2 ===\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm перед головой ===\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        positions = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_embedding(idx) + self.position_embedding(positions)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm перед головой ===\n",
    "        x = self.final_ln(x)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: расчет loss при наличии targets ===\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    # === Добавлено этап 9: гибкая генерация (temperature, top_k) ===\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature: float = 1.0, top_k: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Генерирует `max_new_tokens`, начиная с `idx` (shape [B, T]).\n",
    "        `temperature` < 1.0 делает вывод более «консервативным`, >1.0 — более «креативным`.\n",
    "        `top_k` ограничивает выборку k наиболее вероятными токенами (top‑k sampling).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # масштабируем температуру\n",
    "            if top_k is not None:\n",
    "                # оставим только top_k наивысших логитов, остальные -∞\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_topk = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < min_topk, torch.full_like(logits, float('-inf')), logits)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    # === Конец этапа 9 ===\n",
    "\n",
    "# Пример использования\n",
    "model = MiniGPT(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=4)\n",
    "tokens = torch.randint(0, 1000, (2, 5))\n",
    "targets = torch.randint(0, 1000, (2, 5))\n",
    "logits, loss = model(tokens, targets)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Пример гибкой генерации (temperature=0.8, top_k=40)\n",
    "generated = model.generate(tokens[:, :2], max_new_tokens=5, temperature=0.8, top_k=40)\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc9c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
