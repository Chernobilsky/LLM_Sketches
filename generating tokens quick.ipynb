{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5043b8a3",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e221c438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:37:49.019139Z",
     "start_time": "2025-08-02T13:36:27.146762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT 1: Write me a code for sorting an array in Python ===\n",
      "\n",
      "--- concatenated top‑1 continuation (n = steps) ---\n",
      "[original] \n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Sure! Here's a simple Python code to sort an array:\n",
      "\n",
      "```python\n",
      "# Example usage\n",
      "arr = [5, 2, 8, 0, 1]\n",
      "\n",
      "# Sort the array in ascending order\n",
      "\n",
      "\n",
      "[префикс вопрос] Sure! Here's a simple example of how to sort an array in Python:\n",
      "\n",
      "```python\n",
      "# Example array\n",
      "my_array = [5, 2, 8, 0, 1, 4]\n",
      "\n",
      "# Sort the array in ascending\n",
      "\n",
      "вся тетрадка заняла 1 минут 22 секунд\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# <h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "# <div class=\"toc\"><ul class=\"toc-item\"></ul></div>\n",
    "# Это мы пробуем с Qwen3\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "    \n",
    "import time as tm\n",
    "t1_itogo = tm.time() # длЯ расчета времени исполнения всей тетрадки\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  #убрали ограничение на число строк при отображении датафрейма\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    HAVE_DISPLAY = True\n",
    "except Exception:\n",
    "    HAVE_DISPLAY = False\n",
    "    \n",
    "    \n",
    "import time as tm\n",
    "t1_itogo = tm.time() # длЯ расчета времени исполнения всей тетрадки\n",
    "\n",
    "\n",
    "\n",
    "USE_FILE_PROMPTS = False\n",
    "PROMPT_FILE = \"prompt.txt\"\n",
    "PROMPTS = (\n",
    "    [Path(PROMPT_FILE).read_text(encoding=\"utf-8\").strip()] if USE_FILE_PROMPTS else [\n",
    "        \"Write me a code for sorting an array in Python\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Qwen3 chat template (для instruct-моделей)\n",
    "USE_CHAT_TEMPLATE = True   # <<< добавлено: применять чат-шаблон, если он есть у токенизатора\n",
    "QWEN_THINKING = False      # <<< добавлено: отключаем \"thinking\" для детерминированного greedy\n",
    "\n",
    "\n",
    "MODS = [\n",
    "    (\"original\", lambda p: p),\n",
    "   # (\"typo first e\", lambda p: p.replace(\"e\", \"3\", 1)),\n",
    "  #  (\"add salutation\", lambda p: \"Dear user, \" + p),\n",
    "  #  (\"префикс ======\", lambda p: \"=\" * 10 + p),\n",
    "    (\"префикс вопрос\", lambda p: \"I have a question. \" + p),\n",
    "  #  (\"суффикс 10 лет?\", lambda p: p + \" in the next decade?\"),\n",
    " \n",
    "]\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # <<< изменено: был \"gpt2\"\n",
    "STEPS = 50\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1.  Loading\n",
    "# --------------------------------------------------\n",
    "\n",
    "def setup_model(model_name: str = \"gpt2\", device: Optional[str] = None, dtype: Optional[torch.dtype] = None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    # <<< изменено: позволяем HF выбрать тип автоматически (BF16/FP16 на GPU, FP32 на CPU)\n",
    "    mk = {\"torch_dtype\": dtype or \"auto\"}\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **mk).to(device)\n",
    "    if device == \"cuda\":\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    return mdl, tok, device\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2.  Batch preparation\n",
    "# --------------------------------------------------\n",
    "\n",
    "def prepare_batch(\n",
    "    prompts: List[str],\n",
    "    mods: List[Tuple[str, Callable[[str], str]]],\n",
    "    tokenizer,\n",
    "    device: str,\n",
    "):\n",
    "    texts, meta = [], []  # meta: (prompt_idx, mod_name)\n",
    "    for i, p in enumerate(prompts):\n",
    "        for m_name, m_fn in mods:\n",
    "            meta.append((i, m_name))\n",
    "            txt = m_fn(p)\n",
    "            # <<< добавлено: применяем чат-шаблон Qwen3 при наличии\n",
    "            if USE_CHAT_TEMPLATE and hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "                messages = [{\"role\": \"user\", \"content\": txt}]\n",
    "                txt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    enable_thinking=QWEN_THINKING,\n",
    "                )\n",
    "            texts.append(txt)\n",
    "    enc = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    lens = enc[\"attention_mask\"].sum(dim=1)\n",
    "    return enc, lens, meta\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.  Generation wrapper\n",
    "# --------------------------------------------------\n",
    "\n",
    "def run_generation(model, enc, steps: int, pad_id: int):\n",
    "    cfg = GenerationConfig(max_new_tokens=steps, do_sample=False, num_beams=1, pad_token_id=pad_id, use_cache=True)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**enc, generation_config=cfg)\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4.  Post‑processing\n",
    "# --------------------------------------------------\n",
    "\n",
    "def decode_results(\n",
    "    out, lens, meta, tokenizer, steps: int, decode_tokens: bool\n",
    ") -> Dict[int, Dict[str, List]]:\n",
    "    res: Dict[int, Dict[str, List]] = {}\n",
    "    for idx, (p_idx, m_name) in enumerate(meta):\n",
    "        start = lens[idx].item()\n",
    "        ids = out[idx][start : start + steps].tolist()\n",
    "        tokens = [tokenizer.decode([i], skip_special_tokens=True) for i in ids] if decode_tokens else ids\n",
    "        res.setdefault(p_idx, {})[m_name] = tokens\n",
    "    return res\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5.  Pretty printing\n",
    "# --------------------------------------------------\n",
    "\n",
    "def show_tables(res, prompts, trunc: int, return_dfs: bool, print_text: bool = True, print_table: bool = False): \n",
    "    #если print_text: bool = True, то делаем печать помимо DataFrame\n",
    "    dfs = {}\n",
    "    for i, p in enumerate(prompts):\n",
    "        df = pd.DataFrame(res[i])\n",
    "        df.index = [f\"step {j+1}\" for j in range(len(df))]\n",
    "        hdr = f\"=== PROMPT {i+1}: {p[:trunc]}{'…' if len(p) > trunc else ''} ===\"\n",
    "        \n",
    "\n",
    "\n",
    "        # Таблица top‑1 токенов\n",
    "        if print_table:  # <<< добавлено: можно отключить печать таблицы\n",
    "            print(\"\\n\" + hdr)\n",
    "            if HAVE_DISPLAY:\n",
    "                display(df)\n",
    "            else:\n",
    "                print(df.to_string())   \n",
    "   \n",
    "        # Краткий текст‑продолжение из n токенов\n",
    "        \n",
    "        if print_text:\n",
    "            print(\"\\n\" + hdr)\n",
    "            print(\"\\n--- concatenated top‑1 continuation (n = steps) ---\")\n",
    "            for col in df.columns:\n",
    "                joined = ''.join(res[i][col])  # токены могут содержать пробелы\n",
    "                print(f\"[{col}] {joined}\")\n",
    "                print()\n",
    "\n",
    "        if return_dfs:\n",
    "            dfs[i] = df\n",
    "    return dfs if return_dfs else None\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6.  Public API\n",
    "# --------------------------------------------------\n",
    "\n",
    "def greedy_top1(\n",
    "    prompts: List[str],\n",
    "    mods: List[Tuple[str, Callable[[str], str]]],\n",
    "    model, tokenizer,\n",
    "    steps: int = 40,\n",
    "    device: Optional[str] = None,\n",
    "    decode_tokens: bool = True,\n",
    "    trunc_print: int = 120,\n",
    "    return_dataframes: bool = True,\n",
    "    print_text: bool = True,      # <<< добавлено: пробрасываем флаг печати текста\n",
    "    print_table: bool = False,     # <<< добавлено: пробрасываем флаг печати таблицы\n",
    "):\n",
    "    device = device or model.device\n",
    "    enc, lens, meta = prepare_batch(prompts, mods, tokenizer, device)\n",
    "    out = run_generation(model, enc, steps, tokenizer.pad_token_id)\n",
    "    res = decode_results(out, lens, meta, tokenizer, steps, decode_tokens)\n",
    "    dfs = show_tables(res, prompts, trunc_print, return_dataframes)\n",
    "    return (res, dfs) if return_dataframes else res\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7.  Script example\n",
    "# --------------------------------------------------\n",
    "\n",
    "mdl, tok, dev = setup_model(MODEL_NAME)\n",
    "_ = greedy_top1(PROMPTS, MODS, mdl, tok, steps=STEPS, device=dev)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "t2_itogo = tm.time()\n",
    "print('вся тетрадка заняла', round(t2_itogo - t1_itogo)//60,'минут', round(t2_itogo - t1_itogo)%60,'секунд')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf1f04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:31:04.808456Z",
     "start_time": "2025-08-02T13:31:01.253278Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT 1: Write me a code for sorting an array in Python ===\n",
      "\n",
      "--- concatenated top‑1 continuation (n = steps) ---\n",
      "[original]  sorting an array in Python.\n",
      "\n",
      "import random\n",
      "\n",
      "[префикс вопрос] .\n",
      "\n",
      "I have a question. Write me\n",
      "\n",
      "вся тетрадка заняла 0 минут 4 секунд\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# <h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "# <div class=\"toc\"><ul class=\"toc-item\"></ul></div>\n",
    "# Это мы пробуем с gpt2\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "    \n",
    "import time as tm\n",
    "t1_itogo = tm.time() # длЯ расчета времени исполнения всей тетрадки\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  #убрали ограничение на число строк при отображении датафрейма\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    HAVE_DISPLAY = True\n",
    "except Exception:\n",
    "    HAVE_DISPLAY = False\n",
    "    \n",
    "    \n",
    "import time as tm\n",
    "t1_itogo = tm.time() # длЯ расчета времени исполнения всей тетрадки\n",
    "\n",
    "\n",
    "\n",
    "USE_FILE_PROMPTS = False\n",
    "PROMPT_FILE = \"prompt.txt\"\n",
    "PROMPTS = (\n",
    "    [Path(PROMPT_FILE).read_text(encoding=\"utf-8\").strip()] if USE_FILE_PROMPTS else [\n",
    "        \"Write me a code for sorting an array in Python\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Qwen3 chat template (для instruct-моделей)\n",
    "USE_CHAT_TEMPLATE = True   # <<< добавлено: применять чат-шаблон, если он есть у токенизатора\n",
    "QWEN_THINKING = False      # <<< добавлено: отключаем \"thinking\" для детерминированного greedy\n",
    "\n",
    "\n",
    "MODS = [\n",
    "    (\"original\", lambda p: p),\n",
    "   # (\"typo first e\", lambda p: p.replace(\"e\", \"3\", 1)),\n",
    "  #  (\"add salutation\", lambda p: \"Dear user, \" + p),\n",
    "  #  (\"префикс ======\", lambda p: \"=\" * 10 + p),\n",
    "    (\"префикс вопрос\", lambda p: \"I have a question. \" + p),\n",
    "  #  (\"суффикс 10 лет?\", lambda p: p + \" in the next decade?\"),\n",
    " \n",
    "]\n",
    "\n",
    "MODEL_NAME = \"gpt2\"  # <<< изменено: был \"gpt2\"\n",
    "STEPS = 10\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1.  Loading\n",
    "# --------------------------------------------------\n",
    "\n",
    "def setup_model(model_name: str = \"gpt2\", device: Optional[str] = None, dtype: Optional[torch.dtype] = None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    # <<< изменено: позволяем HF выбрать тип автоматически (BF16/FP16 на GPU, FP32 на CPU)\n",
    "    mk = {\"torch_dtype\": dtype or \"auto\"}\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **mk).to(device)\n",
    "    if device == \"cuda\":\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    return mdl, tok, device\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2.  Batch preparation\n",
    "# --------------------------------------------------\n",
    "\n",
    "def prepare_batch(\n",
    "    prompts: List[str],\n",
    "    mods: List[Tuple[str, Callable[[str], str]]],\n",
    "    tokenizer,\n",
    "    device: str,\n",
    "):\n",
    "    texts, meta = [], []  # meta: (prompt_idx, mod_name)\n",
    "    for i, p in enumerate(prompts):\n",
    "        for m_name, m_fn in mods:\n",
    "            meta.append((i, m_name))\n",
    "            txt = m_fn(p)\n",
    "            # <<< добавлено: применяем чат-шаблон Qwen3 при наличии\n",
    "            if USE_CHAT_TEMPLATE and hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "                messages = [{\"role\": \"user\", \"content\": txt}]\n",
    "                txt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    enable_thinking=QWEN_THINKING,\n",
    "                )\n",
    "            texts.append(txt)\n",
    "    enc = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    lens = enc[\"attention_mask\"].sum(dim=1)\n",
    "    return enc, lens, meta\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.  Generation wrapper\n",
    "# --------------------------------------------------\n",
    "\n",
    "def run_generation(model, enc, steps: int, pad_id: int):\n",
    "    cfg = GenerationConfig(max_new_tokens=steps, do_sample=False, num_beams=1, pad_token_id=pad_id, use_cache=True)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**enc, generation_config=cfg)\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4.  Post‑processing\n",
    "# --------------------------------------------------\n",
    "\n",
    "def decode_results(\n",
    "    out, lens, meta, tokenizer, steps: int, decode_tokens: bool\n",
    ") -> Dict[int, Dict[str, List]]:\n",
    "    res: Dict[int, Dict[str, List]] = {}\n",
    "    for idx, (p_idx, m_name) in enumerate(meta):\n",
    "        start = lens[idx].item()\n",
    "        ids = out[idx][start : start + steps].tolist()\n",
    "        tokens = [tokenizer.decode([i], skip_special_tokens=True) for i in ids] if decode_tokens else ids\n",
    "        res.setdefault(p_idx, {})[m_name] = tokens\n",
    "    return res\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5.  Pretty printing\n",
    "# --------------------------------------------------\n",
    "\n",
    "def show_tables(res, prompts, trunc: int, return_dfs: bool, print_text: bool = True, print_table: bool = False,\n",
    "               model_name: Optional[str] = None, device: Optional[str] = None,\n",
    "                steps_value: Optional[int] = None): \n",
    "    #если print_text: bool = True, то делаем печать помимо DataFrame\n",
    "    dfs = {}\n",
    "    # <<< добавлено: печать информации о модели один раз\n",
    "    if model_name or device:\n",
    "        print(f\"=== MODEL: {model_name or 'unknown'} | device: {device or 'unknown'} ===\")\n",
    "    for i, p in enumerate(prompts):\n",
    "        df = pd.DataFrame(res[i])\n",
    "        df.index = [f\"step {j+1}\" for j in range(len(df))]\n",
    "        hdr = f\"=== PROMPT {i+1}: {p[:trunc]}{'…' if len(p) > trunc else ''} ===\"\n",
    "        \n",
    "\n",
    "\n",
    "        # Таблица top‑1 токенов\n",
    "        if print_table:  # <<< добавлено: можно отключить печать таблицы\n",
    "            print(\"\\n\" + hdr)\n",
    "            if HAVE_DISPLAY:\n",
    "                display(df)\n",
    "            else:\n",
    "                print(df.to_string())   \n",
    "   \n",
    "        # Краткий текст‑продолжение из n токенов\n",
    "        \n",
    "        if print_text:\n",
    "            print(\"\\n\" + hdr)\n",
    "            print(\"\\n--- concatenated top‑1 continuation (n = steps) ---\")\n",
    "            for col in df.columns:\n",
    "                joined = ''.join(res[i][col])  # токены могут содержать пробелы\n",
    "                print(f\"[{col}] {joined}\")\n",
    "                print()\n",
    "\n",
    "        if return_dfs:\n",
    "            dfs[i] = df\n",
    "    return dfs if return_dfs else None\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6.  Public API\n",
    "# --------------------------------------------------\n",
    "\n",
    "def greedy_top1(\n",
    "    prompts: List[str],\n",
    "    mods: List[Tuple[str, Callable[[str], str]]],\n",
    "    model, tokenizer,\n",
    "    steps: int = 40,\n",
    "    device: Optional[str] = None,\n",
    "    decode_tokens: bool = True,\n",
    "    trunc_print: int = 120,\n",
    "    return_dataframes: bool = True,\n",
    "    print_text: bool = True,      # <<< добавлено: пробрасываем флаг печати текста\n",
    "    print_table: bool = False,     # <<< добавлено: пробрасываем флаг печати таблицы\n",
    "):\n",
    "    device = device or model.device\n",
    "    enc, lens, meta = prepare_batch(prompts, mods, tokenizer, device)\n",
    "    out = run_generation(model, enc, steps, tokenizer.pad_token_id)\n",
    "    res = decode_results(out, lens, meta, tokenizer, steps, decode_tokens)\n",
    "     # <<< добавлено: передаём сведения о модели/устройстве и число шагов\n",
    "    model_name = getattr(model, 'name_or_path', None) or (MODEL_NAME if 'MODEL_NAME' in globals() else None)\n",
    "    dfs = show_tables(res, prompts, trunc_print, return_dataframes)\n",
    "    return (res, dfs) if return_dataframes else res\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7.  Script example\n",
    "# --------------------------------------------------\n",
    "\n",
    "mdl, tok, dev = setup_model(MODEL_NAME)\n",
    "_ = greedy_top1(PROMPTS, MODS, mdl, tok, steps=STEPS, device=dev)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "t2_itogo = tm.time()\n",
    "print('вся тетрадка заняла', round(t2_itogo - t1_itogo)//60,'минут', round(t2_itogo - t1_itogo)%60,'секунд')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69769577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
