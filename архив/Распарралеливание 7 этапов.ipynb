{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e84c47",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfdf5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:25:56.265530Z",
     "start_time": "2025-06-20T16:25:56.111510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 5, 1000])\n",
      "Loss: 44.29896926879883\n",
      "Generated tokens: tensor([[477, 265, 265, 265, 265, 265, 265],\n",
      "        [909, 930, 930, 930, 930, 930, 930]])\n"
     ]
    }
   ],
   "source": [
    "# это у нас без 8 этапа, то есть без распаллалеливания\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    # === Добавлено на этапе 3: собственная реализация каузального внимания ===\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "    # === Конец добавленного на этапе 3 кода ===\n",
    "\n",
    "# === Добавлено на этапе 4: MLP-блок после attention ===\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "# === Конец добавленного на этапе 4 кода ===\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_seq_len=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # === Добавлено на этапе 2: позиционные эмбеддинги ===\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # === Конец добавленного на этапе 2 кода ===\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm ===\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        # === Добавлено на этапе 7: Weight tying ===\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size()\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm перед головой ===\n",
    "        x = self.final_ln(x)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: расчет loss при наличии targets ===\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    # === Добавлено на этапе 7: функция генерации текста ===\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    # === Конец функции генерации ===\n",
    "\n",
    "# Пример использования\n",
    "model = MiniGPT(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=4)\n",
    "tokens = torch.randint(0, 1000, (2, 5))\n",
    "targets = torch.randint(0, 1000, (2, 5))\n",
    "logits, loss = model(tokens, targets)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Пример генерации\n",
    "generated = model.generate(tokens[:, :2], max_new_tokens=5)\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9256bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:11:37.135086Z",
     "start_time": "2025-06-20T16:11:32.412896Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 5, 1000])\n",
      "Loss: 45.9717903137207\n",
      "Generated tokens: tensor([[266, 154, 154, 154, 154, 154, 154],\n",
      "        [682, 163, 163, 163, 163, 163, 163]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    # === Добавлено на этапе 3: собственная реализация каузального внимания ===\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "    # === Конец добавленного на этапе 3 кода ===\n",
    "\n",
    "# === Добавлено на этапе 4: MLP-блок после attention ===\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "# === Конец добавленного на этапе 4 кода ===\n",
    "\n",
    "# === Добавлено как модификация на этапе 8: параллельное ветвление и сверка ===\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = FeedForward(embed_dim)\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
    "        self.threshold = 1.0  # можно регулировать чувствительность сверки\n",
    "\n",
    "    def forward(self, x):\n",
    "        # параллельные ветви: attention и mlp\n",
    "        a = self.attn(self.ln_1(x))\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "\n",
    "        # сверка: если сильно различаются, подавим результат\n",
    "        diff = torch.norm(a - m, dim=-1, keepdim=True)  # [B, T, 1]\n",
    "        mask = (diff < self.threshold).float()\n",
    "        combined = mask * (a + m) / 2  # если различаются сильно, обнулим\n",
    "\n",
    "        return x + combined\n",
    "# === Конец модификации на этапе 8 ===\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_seq_len=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # === Добавлено на этапе 2: позиционные эмбеддинги ===\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # === Конец добавленного на этапе 2 кода ===\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm ===\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        # === Добавлено на этапе 7: Weight tying ===\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size()\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm перед головой ===\n",
    "        x = self.final_ln(x)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: расчет loss при наличии targets ===\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    # === Добавлено на этапе 7: функция генерации текста ===\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    # === Конец функции генерации ===\n",
    "\n",
    "# Пример использования\n",
    "model = MiniGPT(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=4)\n",
    "tokens = torch.randint(0, 1000, (2, 5))\n",
    "targets = torch.randint(0, 1000, (2, 5))\n",
    "logits, loss = model(tokens, targets)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Пример генерации\n",
    "generated = model.generate(tokens[:, :2], max_new_tokens=5)\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# а это у нас уже с 8 и с 9 этапом\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    # === Добавлено на этапе 3: собственная реализация каузального внимания ===\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "    # === Конец добавленного на этапе 3 кода ===\n",
    "\n",
    "# === Добавлено на этапе 4: MLP-блок после attention ===\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "# === Конец добавленного на этапе 4 кода ===\n",
    "\n",
    "# === Модифицировано на этапе 9: параллельные блоки attention+MLP ===\n",
    "class TransformerParallelBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = FeedForward(embed_dim)\n",
    "        self.ln_attn = nn.LayerNorm(embed_dim)\n",
    "        self.ln_mlp = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attn(self.ln_attn(x))\n",
    "        m = self.mlp(self.ln_mlp(x))\n",
    "        return x + 0.5 * (a + m)\n",
    "# === Конец изменения на этапе 9 ===\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_seq_len=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # === Добавлено на этапе 2: позиционные эмбеддинги ===\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # === Конец добавленного на этапе 2 кода ===\n",
    "\n",
    "        # === Модифицировано на этапе 9: стек параллельных блоков ===\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerParallelBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        # === Конец модификации на этапе 9 ===\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm ===\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        # === Добавлено на этапе 7: Weight tying ===\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size()\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: финальный LayerNorm перед головой ===\n",
    "        x = self.final_ln(x)\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # === Добавлено на этапе 7: расчет loss при наличии targets ===\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "        # === Конец добавленного на этапе 7 кода ===\n",
    "\n",
    "    # === Добавлено на этапе 7: функция генерации текста ===\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    # === Конец функции генерации ===\n",
    "\n",
    "# Пример использования\n",
    "model = MiniGPT(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=4)\n",
    "tokens = torch.randint(0, 1000, (2, 5))\n",
    "targets = torch.randint(0, 1000, (2, 5))\n",
    "logits, loss = model(tokens, targets)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Пример генерации\n",
    "generated = model.generate(tokens[:, :2], max_new_tokens=5)\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
