{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a79603b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbfa445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:47:27.280229Z",
     "start_time": "2025-08-01T15:47:15.264438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value.\n",
      "\n",
      "I've been a researcher for twenty years, and I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never\n"
     ]
    }
   ],
   "source": [
    "# тут gpt2\n",
    "# не помню, чем отличается первый бюлок от второго, надо еше посмогтреть\n",
    "\n",
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"gpt2\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False):\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"gpt2\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\"\"\"\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "    if use_chat:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    else:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"gpt2\") or \"openai-community/gpt2\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=gpt2\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e61d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:51:36.213031Z",
     "start_time": "2025-08-01T15:51:31.945484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using PROMPT.TXT from current directory.\n",
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value.\n",
      "\n",
      "I've been a researcher for twenty years, and I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never been a scientist. I've never\n"
     ]
    }
   ],
   "source": [
    "# тут gpt2\n",
    "\n",
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3\": \"Qwen/Qwen3-0.6B\",  # [ADD] alias for convenience\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"gpt2\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False, trust_remote_code: bool = False):  # [MOD] added trust_remote_code\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only, trust_remote_code=trust_remote_code)  # [MOD] pass trust_remote_code\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "        trust_remote_code=trust_remote_code,  # [MOD]\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"gpt2\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\"\"\"\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "    if use_chat:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    else:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"gpt2\") or \"openai-community/gpt2\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=gpt2\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [ADD] allow remote code (needed by some repos)\n",
    "    p.add_argument(\"--trust-remote-code\", action=\"store_true\", help=\"Allow custom modeling code from repo (use only for trusted repos)\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=args.trust_remote_code)  # [MOD]\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=args.trust_remote_code,  # [MOD]\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if args.print_tokens:\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41f1f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T16:26:48.728675Z",
     "start_time": "2025-08-01T16:24:43.858506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] Auto-enabling trust_remote_code for Qwen model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "модель  Qwen/Qwen3-0.6B\n",
      "[info] Using PROMPT.TXT from current directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[warn] Chat template requires jinja2>=3.1; falling back to plain prompt. Error: module 'jinja2' has no attribute 'pass_eval_context'\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overdetermined reasons, I’ve lately found the world an increasingly terrifying and depressing place. It’s gotten harder and harder to concentrate on research, or even popular science writing. Every so often, though, something breaks through that wakes my inner child, reminds me of why I fell in love with research thirty years ago, and helps me forget about the triumphantly strutting factions working to destroy everything I value. That something is a story. And I’ve been writing stories about the world, but I’m not sure if I’m doing it right. I’m not sure if I’m writing about the world or about myself. And I’m not sure if I’m writing about the world in a way that’s meaningful or just a\n",
      "\n",
      "--- stats ---\n",
      "device: cpu\n",
      "elapsed_s: 86.528\n",
      "new_tokens: 64\n",
      "tokens_per_s: 0.74\n"
     ]
    }
   ],
   "source": [
    "# по умолчанию Qwen3\n",
    "\n",
    "import argparse, time, re, sys, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# [ADD] Py3.8/3.9 typing support\n",
    "from typing import Optional\n",
    "# [ADD] Read prompt from a file path\n",
    "from pathlib import Path\n",
    "\n",
    "PRESETS = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"qwen3\": \"Qwen/Qwen3-0.6B\",  # [ADD] alias for convenience\n",
    "    \"qwen3-0.6b\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"nemotron-1.5b\": \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"auto\": \"auto\",\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "# [ADD] Public API: загрузка модели и один запуск генерации из Python\n",
    "def load_model(model: str = \"qwen3\", device: Optional[str] = None, dtype: str = \"auto\", local_files_only: bool = False, trust_remote_code: Optional[bool] = None):  # [MOD] default model qwen3; infer trust by model if None  # [MOD] added trust_remote_code\n",
    "    \"\"\"[API] Загрузить токенайзер и модель. Возвращает (tokenizer, model, device).\n",
    "    Пример:\n",
    "        tok, mdl, dev = load_model(\"gpt2\", device=\"cpu\")\n",
    "    \"\"\"\n",
    "    print(\"используем модель\", model)\n",
    "    model_id = PRESETS.get(model.lower(), model)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "    torch_dtype = DTYPE_MAP[dtype]\n",
    "\n",
    "    # [ADD] If trust_remote_code is not specified, auto-enable for Qwen models\n",
    "    if trust_remote_code is None:\n",
    "        try:\n",
    "            if (model.lower().startswith(\"qwen\") or \"Qwen/\" in model_id):\n",
    "                trust_remote_code = True\n",
    "        except Exception:\n",
    "            trust_remote_code = False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=local_files_only, trust_remote_code=trust_remote_code)  # [MOD] pass trust_remote_code\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_obj = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=local_files_only,\n",
    "        trust_remote_code=trust_remote_code,  # [MOD]\n",
    "    ).eval().to(device)\n",
    "\n",
    "    return tokenizer, model_obj, device\n",
    "\n",
    "# [ADD] [API] Один запуск генерации поверх уже загруженных токенайзера/модели\n",
    "def generate_once(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    *,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    warmup: bool = True,\n",
    "):\n",
    "    \"\"\"[API] Сгенерировать текст. Возвращает (text, stats_dict).\n",
    "    Пример:\n",
    "        text, stats = generate_once(tok, mdl, prompt=\"Hello\", max_new_tokens=16)\n",
    "    \"\"\"\n",
    "    if prompt is None and prompt_file:\n",
    "        try:\n",
    "            prompt = Path(prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read prompt_file '{prompt_file}': {e}\")\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "\n",
    "    # Подготовка входа\n",
    "    inputs = build_inputs(tokenizer, prompt, system=system, enable_thinking=thinking, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Прогрев (минимальный)\n",
    "    if warmup:\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    if (device or (torch.cuda.is_available() and model.device.type == \"cuda\")) and model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if model.device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    new_tokens = int(out.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    stats = {\n",
    "        \"device\": str(model.device),\n",
    "        \"elapsed_s\": dt,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"tokens_per_s\": (new_tokens / dt) if dt > 0 else float(\"inf\"),\n",
    "    }\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] [API] Удобная обёртка: загрузка + генерация за один вызов\n",
    "def generate_text(\n",
    "    *,\n",
    "    model: str = \"qwen3\",\n",
    "    device: Optional[str] = None,\n",
    "    dtype: str = \"auto\",\n",
    "    local_files_only: bool = False,\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_file: Optional[str] = None,\n",
    "    system: Optional[str] = None,\n",
    "    thinking: bool = False,\n",
    "    strip_think: bool = False,\n",
    "    max_new_tokens: int = 64,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    trust_remote_code: Optional[bool] = None,\n",
    "):\n",
    "    tok, mdl, dev = load_model(model=model, device=device, dtype=dtype, local_files_only=local_files_only, trust_remote_code=trust_remote_code)\n",
    "    text, stats = generate_once(\n",
    "        tok,\n",
    "        mdl,\n",
    "        prompt=prompt,\n",
    "        prompt_file=prompt_file,\n",
    "        system=system,\n",
    "        thinking=thinking,\n",
    "        strip_think=strip_think,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        device=dev,\n",
    "    )\n",
    "    return text, stats\n",
    "\n",
    "# [ADD] Helpers to make the script robust in notebooks / Jupyter where argv contains ipykernel args to make the script robust in notebooks / Jupyter where argv contains ipykernel args\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# [ADD] Safe prompt reader that won't block in notebooks when stdin is not provided\n",
    "# [MOD] Py3.8/3.9 compatible typing (PEP 604 not available); use Optional[str]\n",
    "def _read_prompt_or_default(arg_prompt: Optional[str]) -> str:\n",
    "    if arg_prompt is not None:\n",
    "        return arg_prompt\n",
    "    # Try to read from stdin only if data is available; otherwise fall back to empty prompt\n",
    "    try:\n",
    "        if sys.stdin and not sys.stdin.isatty():\n",
    "            data = sys.stdin.read()\n",
    "            if data:\n",
    "                return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: empty prompt (safe for generation) with a short notice printed by caller\n",
    "    return \"\"\n",
    "\n",
    "def build_inputs(tokenizer, prompt, system=None, enable_thinking=None, device=\"cpu\"):\n",
    "    \"\"\"Return input_ids tensor on the target device, using chat template if present.\n",
    "    Надёжно обрабатывает отсутствие jinja2 (требуется для chat_template):\n",
    "    при ошибке откатывается к обычной подаче prompt без шаблона и печатает предупреждение.\n",
    "    \"\"\"\n",
    "    model_inputs = None\n",
    "    use_chat = getattr(tokenizer, \"chat_template\", None) not in (None, \"\", False)\n",
    "\n",
    "    if use_chat:\n",
    "        try:\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            # enable_thinking is used by Qwen3; harmless for tokenizers that ignore extra vars\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=enable_thinking if enable_thinking is not None else False,\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "        except (ImportError, AttributeError) as e:\n",
    "            # [WARN] jinja2 старой версии или отсутствует → откатываемся на простой prompt\n",
    "            print(\"[warn] Chat template requires jinja2>=3.1; falling back to plain prompt. Error:\", e, file=sys.stderr)\n",
    "            use_chat = False\n",
    "        except Exception as e:\n",
    "            # Любая другая ошибка рендера — также откат на простой prompt\n",
    "            print(\"[warn] Failed to render chat template; falling back to plain prompt. Error:\", e, file=sys.stderr)\n",
    "            use_chat = False\n",
    "\n",
    "    if not use_chat or model_inputs is None:\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        # [ADD] Ensure non-empty input for decoder-only models when prompt is empty\n",
    "        if model_inputs[\"input_ids\"].shape[1] == 0:\n",
    "            fallback_id = tokenizer.eos_token_id\n",
    "            if fallback_id is None:\n",
    "                fallback_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            model_inputs = {\n",
    "                \"input_ids\": torch.tensor([[fallback_id]], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Simple local HF generation harness\")\n",
    "    # [MOD] Make --model optional with a sensible default so the script runs inside notebooks\n",
    "    p.add_argument(\n",
    "        \"--model\",\n",
    "        default=(PRESETS.get(\"qwen3\") or \"Qwen/Qwen3-0.6B\"),\n",
    "        help=\"preset key (gpt2|qwen3-0.6b|nemotron-1.5b) or any HF repo id; default=qwen3\",\n",
    "    )\n",
    "    p.add_argument(\"--device\", choices=[\"cpu\", \"cuda\"], default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    p.add_argument(\"--dtype\", choices=list(DTYPE_MAP.keys()), default=\"auto\",\n",
    "                   help=\"torch dtype for model weights\")\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=64)\n",
    "    p.add_argument(\"--temperature\", type=float, default=0.0, help=\"0.0 => greedy\")\n",
    "    p.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    p.add_argument(\"--do-sample\", action=\"store_true\", help=\"enable sampling (else greedy)\")\n",
    "    p.add_argument(\"--thinking\", action=\"store_true\",\n",
    "                   help=\"For Qwen3: enable thinking mode (adds <think>...</think> content)\")\n",
    "    p.add_argument(\"--strip-think\", action=\"store_true\",\n",
    "                   help=\"Strip <think>...</think> block from decoded output (if present)\")\n",
    "    p.add_argument(\"--system\", default=None, help=\"Optional system prompt for chat models\")\n",
    "    p.add_argument(\"--prompt\", default=None, help=\"Prompt; if omitted, read from stdin; in notebooks defaults to empty string\")\n",
    "    # [ADD] Read prompt from file\n",
    "    p.add_argument(\"--prompt-file\", default=None, help=\"Path to a text file with the prompt (UTF-8)\")\n",
    "    p.add_argument(\"--print-tokens\", action=\"store_true\", help=\"Also print token counts and toks/sec\")\n",
    "    # [ADD] allow remote code (needed by some repos)\n",
    "    p.add_argument(\"--trust-remote-code\", action=\"store_true\", help=\"Allow custom modeling code from repo (use only for trusted repos)\")\n",
    "    # [MOD] In notebooks, ignore unrelated ipykernel args by parsing only known flags\n",
    "    args = p.parse_args([]) if _in_notebook() else p.parse_args()\n",
    "\n",
    "    model_id = PRESETS.get(args.model.lower(), args.model)\n",
    "    device = args.device\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, falling back to CPU\", file=sys.stderr)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    torch_dtype = DTYPE_MAP[args.dtype]\n",
    "\n",
    "    # [ADD] Auto-enable trust for Qwen models if flag not set\n",
    "    trust_flag = args.trust_remote_code or (\"Qwen/\" in model_id or args.model.lower().startswith(\"qwen\"))\n",
    "    if trust_flag and not args.trust_remote_code:\n",
    "        print(\"[info] Auto-enabling trust_remote_code for Qwen model.\", file=sys.stderr)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_flag)  # [MOD] use inferred flag\n",
    "    # Ensure pad_token_id exists for generation\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(\"модель \", model_id)\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=trust_flag,  # [MOD] use inferred flag\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # [MOD] Prompt resolution priority: --prompt-file > --prompt > stdin > PROMPT.TXT > empty\n",
    "    prompt = None\n",
    "    if args.prompt_file:\n",
    "        try:\n",
    "            prompt = Path(args.prompt_file).read_text(encoding=\"utf-8\").strip()\n",
    "            print(f\"[info] Loaded prompt from file: {args.prompt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read --prompt-file '{args.prompt_file}': {e}\", file=sys.stderr)\n",
    "    if prompt is None and args.prompt is not None:\n",
    "        prompt = args.prompt\n",
    "    if prompt is None:\n",
    "        # try stdin (non-blocking path)\n",
    "        data = _read_prompt_or_default(None)\n",
    "        if data:\n",
    "            prompt = data\n",
    "    if prompt is None and Path(\"PROMPT.TXT\").exists():\n",
    "        try:\n",
    "            prompt = Path(\"PROMPT.TXT\").read_text(encoding=\"utf-8\").strip()\n",
    "            print(\"[info] Using PROMPT.TXT from current directory.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read PROMPT.TXT: {e}\", file=sys.stderr)\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\n",
    "        if _in_notebook():\n",
    "            print(\"[info] No --prompt/--prompt-file/stdin and PROMPT.TXT not found; using empty prompt.\")\n",
    "\n",
    "    inputs = build_inputs(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        system=args.system,\n",
    "        enable_thinking=args.thinking,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=args.do_sample,\n",
    "        temperature=args.temperature if args.do_sample else None,\n",
    "        top_p=args.top_p if args.do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    # Remove None entries (generate() complains)\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    # Warmup (tiny, optional)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**{k: v.clone() for k, v in inputs.items()}, max_new_tokens=1)\n",
    "\n",
    "    # Timed run\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Separate new tokens from the continuation\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    if args.strip_think:\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    if True:  # исходно было args.print_tokens\n",
    "        toks_per_s = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "        print(\"\\n--- stats ---\")\n",
    "        print(f\"device: {device}\")\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                print(f\"gpu: {torch.cuda.get_device_name()}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"elapsed_s: {dt:.3f}\")\n",
    "        print(f\"new_tokens: {new_tokens}\")\n",
    "        print(f\"tokens_per_s: {toks_per_s:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41d1114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T16:32:30.501810Z",
     "start_time": "2025-08-01T16:32:16.179273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\cab\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "CUDA not available, falling back to CPU\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cab\\Мишка\\Matrix model\\hf_local_gen.py\", line 139, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\cab\\Мишка\\Matrix model\\hf_local_gen.py\", line 82, in main\n",
      "    inputs = build_inputs(\n",
      "  File \"C:\\Users\\cab\\Мишка\\Matrix model\\hf_local_gen.py\", line 26, in build_inputs\n",
      "    text = tokenizer.apply_chat_template(\n",
      "  File \"C:\\Users\\cab\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1640, in apply_chat_template\n",
      "    rendered_chat, generation_indices = render_jinja_template(\n",
      "  File \"C:\\Users\\cab\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 481, in render_jinja_template\n",
      "    compiled_template = _compile_jinja_template(chat_template)\n",
      "  File \"C:\\Users\\cab\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 398, in _compile_jinja_template\n",
      "    class AssistantTracker(Extension):\n",
      "  File \"C:\\Users\\cab\\anaconda3\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 414, in AssistantTracker\n",
      "    @jinja2.pass_eval_context\n",
      "AttributeError: module 'jinja2' has no attribute 'pass_eval_context'\n"
     ]
    }
   ],
   "source": [
    "!python hf_local_gen.py --model qwen3-0.6b --device cuda --max-new-tokens 128 --print-tokens --prompt \"write a quicksort function in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb6bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
